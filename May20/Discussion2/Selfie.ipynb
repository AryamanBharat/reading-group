{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Selfie.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfv9ZICvsv5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Libraries\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pickle\n",
        "import inspect\n",
        "import numpy as np\n",
        "import math\n",
        "import logging\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "import matplotlib as mpl\n",
        "if os.environ.get('DISPLAY','') == '':\n",
        "    print('no display found. Using non-interactive Agg backend')\n",
        "    mpl.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.distributed as dist\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES5OON2ysZqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformer\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    ''' Scaled Dot-Product Attention '''\n",
        "\n",
        "    def __init__(self, temperature, attn_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        attn = torch.bmm(q, k.transpose(1, 2))\n",
        "        attn = attn / self.temperature\n",
        "\n",
        "        if mask is not None:\n",
        "            attn = attn.masked_fill(mask, -np.inf)\n",
        "\n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        output = torch.bmm(attn, v)\n",
        "\n",
        "        return output, attn\n",
        "    \n",
        "class MultiHeadAttention(nn.Module):\n",
        "    ''' Multi-Head Attention module '''\n",
        "\n",
        "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_k = d_k\n",
        "        self.d_v = d_v\n",
        "\n",
        "        self.w_qs = nn.Linear(d_model, n_head * d_k)\n",
        "        self.w_ks = nn.Linear(d_model, n_head * d_k)\n",
        "        self.w_vs = nn.Linear(d_model, n_head * d_v)\n",
        "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
        "        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
        "        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5))\n",
        "        self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
        "        nn.init.xavier_normal_(self.fc.weight)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
        "\n",
        "        sz_b, len_q, _ = q.size()\n",
        "        sz_b, len_k, _ = k.size()\n",
        "        sz_b, len_v, _ = v.size()\n",
        "\n",
        "        residual = q\n",
        "\n",
        "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
        "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
        "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
        "\n",
        "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k) # (n*b) x lq x dk\n",
        "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k) # (n*b) x lk x dk\n",
        "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v) # (n*b) x lv x dv\n",
        "        if not mask==None:\n",
        "            mask = mask.repeat(n_head, 1, 1) # (n*b) x .. x ..\n",
        "        output, attn = self.attention(q, k, v, mask=mask)\n",
        "\n",
        "        output = output.view(n_head, sz_b, len_q, d_v)\n",
        "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1) # b x lq x (n*dv)\n",
        "\n",
        "        output = self.dropout(self.fc(output))\n",
        "        output = self.layer_norm(output + residual)\n",
        "\n",
        "        return output, attn\n",
        "    \n",
        "class GELU(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT used the GELU instead of RELU\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rufs4XpTsvzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    ''' A two-feed-forward-layer module '''\n",
        "\n",
        "    def __init__(self, d_in, d_hid, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.w_1 = nn.Conv1d(d_in, d_hid, 1) # position-wise\n",
        "        self.w_2 = nn.Conv1d(d_hid, d_in, 1) # position-wise\n",
        "        self.layer_norm = nn.LayerNorm(d_in)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = GELU()\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        output = x.transpose(1, 2)\n",
        "        output = self.w_2(self.activation(self.w_1(output)))\n",
        "        output = output.transpose(1, 2)\n",
        "        output = self.dropout(output)\n",
        "        output = self.layer_norm(output + residual)\n",
        "        return output\n",
        "    \n",
        "class EncoderLayer(nn.Module):\n",
        "    ''' Compose with two layers '''\n",
        "\n",
        "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.slf_attn = MultiHeadAttention(\n",
        "            n_head, d_model, d_k, d_v, dropout=dropout)\n",
        "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
        "\n",
        "    def forward(self, enc_input, non_pad_mask=None, slf_attn_mask=None):\n",
        "        enc_output, enc_slf_attn = self.slf_attn(\n",
        "            enc_input, enc_input, enc_input, mask=slf_attn_mask)\n",
        "        enc_output *= non_pad_mask\n",
        "\n",
        "        enc_output = self.pos_ffn(enc_output)\n",
        "        enc_output *= non_pad_mask\n",
        "\n",
        "        return enc_output, enc_slf_attn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBt_QKbBs1jQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "\n",
        "class attention_pooling(nn.Module):\n",
        "    ''' A encoder model with self attention mechanism. '''\n",
        "\n",
        "    def __init__(self, n_layers, n_head, d_k, d_v, d_model, d_inner, dropout=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer_stack = nn.ModuleList([EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, src_seq, return_attns=False):\n",
        "\n",
        "        enc_slf_attn_list = []\n",
        "        enc_output = src_seq\n",
        "        \n",
        "        for enc_layer in self.layer_stack:\n",
        "            enc_output, enc_slf_attn = enc_layer(enc_output,non_pad_mask=1,slf_attn_mask=None)\n",
        "            if return_attns:\n",
        "                enc_slf_attn_list += [enc_slf_attn]\n",
        "\n",
        "        if return_attns:\n",
        "            return enc_output, enc_slf_attn_list\n",
        "        return enc_output[:, 0, :]\n",
        "\n",
        "class SelfieModel(nn.Module):\n",
        "    def __init__(self, n_layers, n_heads, d_in, d_model, d_ff, n_split, dropout=0.1, use_cuda=True, gpu = None):\n",
        "        super(SelfieModel, self).__init__()\n",
        "        self.n_split = n_split\n",
        "        self.at_pool = attention_pooling(n_layers + 1, n_heads, d_in, d_in, d_model, d_ff)\n",
        "\n",
        "        if use_cuda:\n",
        "            if gpu is None:\n",
        "                self.row_embeddings = nn.Parameter(torch.randn(n_split, d_model).cuda())\n",
        "                self.column_embeddings = nn.Parameter(torch.zeros(n_split, d_model).cuda())\n",
        "                self.u0 = nn.Parameter(torch.zeros(1,1,d_model).cuda())\n",
        "            else:\n",
        "                self.row_embeddings = nn.Parameter(torch.randn(n_split, d_model).cuda(gpu))\n",
        "                self.column_embeddings = nn.Parameter(torch.zeros(n_split, d_model).cuda(gpu))\n",
        "                self.u0 = nn.Parameter(torch.zeros(1,1,d_model).cuda(gpu))\n",
        "        else:\n",
        "            self.row_embeddings = nn.Parameter(torch.randn(n_split, d_model))\n",
        "            self.column_embeddings = nn.Parameter(torch.zeros(n_split, d_model))\n",
        "            self.u0 = nn.Parameter(torch.zeros(1,1,d_model))\n",
        "\n",
        "    def forward(self, src_seq, pos, return_attns=False):\n",
        "        u = self.u0.repeat((src_seq.shape[0], 1, 1))\n",
        "        src_seq = torch.cat([u, src_seq], dim=1)\n",
        "        before_embeddings =  self.at_pool(src_seq)\n",
        "        final = []\n",
        "        rows = map(lambda x: np.trunc(x / self.n_split).astype(\"int\"), pos)\n",
        "        cols = map(lambda x: np.mod(x, self.n_split).astype(\"int\"), pos)\n",
        "        for (i,j) in zip(rows, cols):\n",
        "            sum_up = before_embeddings + self.row_embeddings[i,:] + self.column_embeddings[j,:]\n",
        "            sum_up = sum_up.unsqueeze(1)\n",
        "            final.append(sum_up)\n",
        "\n",
        "        return torch.cat(final, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CepU3FStDAb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Resnet\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.relu = nn.ReLU(inplace = True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn0 = nn.BatchNorm2d(in_planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        pre = self.relu(self.bn0(x))\n",
        "        out = self.relu(self.bn1(self.conv1(pre)))\n",
        "        out = self.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.conv3(out)\n",
        "        if len(self.shortcut)==0:\n",
        "            out += self.shortcut(x)\n",
        "        else:\n",
        "            out += self.shortcut(pre)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "class P(nn.Module):\n",
        "    def __init__(self, block, num_blocks):\n",
        "        super(P, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return out\n",
        "\n",
        "def get_P_model():\n",
        "    return P(Bottleneck, [3,4,6])\n",
        "\n",
        "def ResNet50(number_classes=10):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], num_classes=number_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C7IE7aPtILf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Miscellaneous\n",
        "\n",
        "class NormalizeByChannelMeanStd(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super(NormalizeByChannelMeanStd, self).__init__()\n",
        "        if not isinstance(mean, torch.Tensor):\n",
        "            mean = torch.tensor(mean)\n",
        "        if not isinstance(std, torch.Tensor):\n",
        "            std = torch.tensor(std)\n",
        "        self.register_buffer(\"mean\", mean)\n",
        "        self.register_buffer(\"std\", std)\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        return normalize_fn(tensor, self.mean, self.std)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'mean={}, std={}'.format(self.mean, self.std)\n",
        "\n",
        "\n",
        "def normalize_fn(tensor, mean, std):\n",
        "    \"\"\"Differentiable version of torchvision.functional.normalize\"\"\"\n",
        "    # here we assume the color channel is in at dim=1\n",
        "    mean = mean[None, :, None, None]\n",
        "    std = std[None, :, None, None]\n",
        "    return tensor.sub(mean).div(std)\n",
        "\n",
        "class stats:\n",
        "    def __init__(self, path, start_epoch):\n",
        "        if start_epoch != 0:\n",
        "           stats_ = sio.loadmat(os.path.join(path,'stats.mat'))\n",
        "           data = stats_['data']\n",
        "           content = data[0,0]\n",
        "           self.trainObj = content['trainObj'][:,:start_epoch].squeeze().tolist()\n",
        "           self.trainTop1 = content['trainTop1'][:,:start_epoch].squeeze().tolist()\n",
        "           self.valObj = content['valObj'][:,:start_epoch].squeeze().tolist()\n",
        "           self.valTop1 = content['valTop1'][:,:start_epoch].squeeze().tolist()\n",
        "\n",
        "           self.avalObj = content['adv_valObj'][:,:start_epoch].squeeze().tolist()\n",
        "           self.avalTop1 = content['adv_prec1'][:,:start_epoch].squeeze().tolist()\n",
        "           if start_epoch == 1:\n",
        "               self.trainObj = [self.trainObj]\n",
        "               self.trainTop1 = [self.trainTop1]\n",
        "               self.valObj = [self.valObj]\n",
        "               self.valTop1 = [self.valTop1]\n",
        "               self.avalObj = [self.avalObj]\n",
        "               self.avalTop1 = [self.avalTop1]\n",
        "        else:\n",
        "           self.trainObj = []\n",
        "           self.trainTop1 = []\n",
        "           self.valObj = []\n",
        "           self.valTop1 = []\n",
        "           self.avalObj = []\n",
        "           self.avalTop1 = []\n",
        "           \n",
        "    def _update(self, trainObj, top1, valObj, prec1, avalObj, aprec1):\n",
        "        self.trainObj.append(trainObj)\n",
        "        self.trainTop1.append(top1.cpu().numpy())\n",
        "        self.valObj.append(valObj)\n",
        "        self.valTop1.append(prec1.cpu().numpy())\n",
        "        self.avalObj.append(avalObj)\n",
        "        self.avalTop1.append(aprec1.cpu().numpy())\n",
        "\n",
        "\n",
        "def plot_curve(stats, path, iserr):\n",
        "    \n",
        "    trainObj = np.array(stats.trainObj)\n",
        "    valObj = np.array(stats.valObj)\n",
        "    avalObj = np.array(stats.avalObj)\n",
        "    if iserr:\n",
        "        trainTop1 = 100 - np.array(stats.trainTop1)\n",
        "        valTop1 = 100 - np.array(stats.valTop1)\n",
        "        avalTop1 = 100 - np.array(stats.avalTop1)\n",
        "        titleName = 'error'\n",
        "    else:\n",
        "        trainTop1 = np.array(stats.trainTop1)\n",
        "        valTop1 = np.array(stats.valTop1)\n",
        "        avalTop1 = np.array(stats.avalTop1)\n",
        "        titleName = 'accuracy'\n",
        "        \n",
        "    epoch = len(trainObj)\n",
        "    figure = plt.figure()\n",
        "    obj = plt.subplot(1,2,1)\n",
        "    obj.plot(range(1,epoch+1),trainObj,'o-',label = 'train')\n",
        "    obj.plot(range(1,epoch+1),valObj,'o-',label = 'val')\n",
        "    obj.plot(range(1,epoch+1),avalObj,'o-',label = 'adv_val')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.title('objective')\n",
        "    handles, labels = obj.get_legend_handles_labels()\n",
        "    obj.legend(handles[::-1], labels[::-1])\n",
        "    top1 = plt.subplot(1,2,2)\n",
        "    top1.plot(range(1,epoch+1),trainTop1,'o-',label = 'train')\n",
        "    top1.plot(range(1,epoch+1),valTop1,'o-',label = 'val')\n",
        "    top1.plot(range(1,epoch+1),avalTop1,'o-',label = 'adv_val')\n",
        "    plt.title('top1'+titleName)\n",
        "    plt.xlabel('epoch')\n",
        "    handles, labels = top1.get_legend_handles_labels()\n",
        "    top1.legend(handles[::-1], labels[::-1])\n",
        "    filename = os.path.join(path, 'net-train.pdf')\n",
        "    figure.savefig(filename, bbox_inches='tight')\n",
        "    plt.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At7VnrcttObM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset\n",
        "\n",
        "class CifarDataset(Dataset):\n",
        "\n",
        "    def __init__(self, _dir, train, transform, percent):\n",
        "\n",
        "        self.dir=osp.join( _dir, 'cifar-10-batches-py')\n",
        "        self.transforms=transform\n",
        "        train_filenames = ['data_batch_{}'.format(ii + 1) for ii in range(5)]\n",
        "        eval_filename = 'test_batch'\n",
        "\n",
        "        if train:\n",
        "            data_images = np.zeros((50000, 32, 32, 3), dtype='uint8')\n",
        "            data_labels = np.zeros(50000, dtype='int32')\n",
        "            for ii, fname in enumerate(train_filenames):\n",
        "                cur_images, cur_labels = self._load_datafile(osp.join(self.dir, fname))\n",
        "                data_images[ii * 10000 : (ii+1) * 10000, ...] = cur_images\n",
        "                data_labels[ii * 10000 : (ii+1) * 10000, ...] = cur_labels\n",
        "            permutation = np.random.permutation(50000)\n",
        "            self.choose_images=data_images[permutation]\n",
        "            self.choose_target=data_labels[permutation]\n",
        "            choose = []\n",
        "            all_indexes = np.array(range(50000))\n",
        "            self.number = int(50000 * percent)\n",
        "            for i in range(10):\n",
        "                indexes = all_indexes[self.choose_target == i]\n",
        "                choose.append(indexes[:int(len(indexes) * percent)])\n",
        "            choose = np.concatenate(choose, 0)\n",
        "            self.choose_images = self.choose_images[choose]\n",
        "            self.choose_target = self.choose_target[choose]\n",
        "        else:\n",
        "            data_images, data_labels = self._load_datafile(osp.join(self.dir, eval_filename))\n",
        "            self.number=int(10000*percent)\n",
        "            permutation = np.random.permutation(10000)\n",
        "            self.choose_images=data_images[permutation[:self.number]]\n",
        "            self.choose_target=data_labels[permutation[:self.number]]\n",
        "            self.number = 10000\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.number\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img=self.choose_images[index]\n",
        "        target = self.choose_target[index]\n",
        "        img = self.transforms(img)\n",
        "\n",
        "        return img,target\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_datafile(filename):\n",
        "        with open(filename, 'rb') as fo:\n",
        "            data_dict = pickle.load(fo, encoding='bytes')\n",
        "            assert data_dict[b'data'].dtype == np.uint8\n",
        "            image_data = data_dict[b'data']\n",
        "            image_data = image_data.reshape((10000, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "            return image_data, np.array(data_dict[b'labels'])\n",
        "        \n",
        "class ImageNetDataset(Dataset):\n",
        "\n",
        "    def __init__(self, _dir, train, transform, percent):\n",
        "\n",
        "        self.dir=_dir\n",
        "        self.transforms=transform\n",
        "        train_filenames = ['train_data_batch_{}'.format(ii + 1) for ii in range(10)]\n",
        "        eval_filename = 'val_data'\n",
        "\n",
        "        if train:\n",
        "            data_images = []\n",
        "            data_labels = []\n",
        "            for ii, fname in enumerate(train_filenames):\n",
        "                cur_images, cur_labels = self._load_datafile(osp.join(self.dir, fname))\n",
        "                data_images.append(cur_images)\n",
        "                data_labels.append(cur_labels)\n",
        "            data_images = np.concatenate(data_images, axis = 0)\n",
        "            data_labels = np.concatenate(data_labels, axis = 0)\n",
        "            n = data_images.shape[0]\n",
        "            permutation = np.random.permutation(n)\n",
        "            self.choose_images=data_images[permutation]\n",
        "            self.choose_target=data_labels[permutation]\n",
        "            choose = []\n",
        "            all_indexes = np.array(range(n))\n",
        "            for i in range(1000):\n",
        "                indexes = all_indexes[self.choose_target == i]\n",
        "                choose.append(indexes[:int(len(indexes) * percent)])\n",
        "            choose = np.concatenate(choose, 0)\n",
        "            self.choose_images = self.choose_images[choose]\n",
        "            self.choose_target = self.choose_target[choose]\n",
        "        else:\n",
        "            data_images, data_labels = self._load_datafile(osp.join(self.dir, eval_filename))\n",
        "            n = data_images.shape[0]\n",
        "            self.number=int(n*percent)\n",
        "            self.choose_images=data_images[:self.number]\n",
        "            self.choose_target=data_labels[:self.number]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.choose_images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.choose_images[index]\n",
        "        target = self.choose_target[index]\n",
        "        img = self.transforms(img)\n",
        "\n",
        "        return img,target -1\n",
        "\n",
        "    @staticmethod\n",
        "    def _load_datafile(filename):\n",
        "        with open(filename, 'rb') as fo:\n",
        "            data_dict = pickle.load(fo, encoding='bytes')\n",
        "            assert data_dict['data'].dtype == np.uint8\n",
        "            image_data = data_dict['data']\n",
        "            image_data = image_data.reshape((image_data.shape[0], 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "            return image_data, np.array(data_dict['labels'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5XgL-h4tV8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arguments\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Selfie')\n",
        "  \n",
        "parser.add_argument('--arch', '-a', metavar='ARCH', default='resnet50',\n",
        "                    help='model architecture: ')\n",
        "parser.add_argument('-j', '--workers', default=8, type=int, metavar='N',\n",
        "                    help='number of data loading workers (default: 4)')\n",
        "parser.add_argument('--epochs', default=200, type=int, metavar='N',\n",
        "                    help='number of steps of selfie')\n",
        "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
        "                    help='manual epoch number (useful on restarts)')\n",
        "parser.add_argument('-b', '--batch-size', default=256, type=int,\n",
        "                    metavar='N', help='mini-batch size (default: 256)')\n",
        "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\n",
        "                    metavar='LR', help='initial learning rate')\n",
        "parser.add_argument('--lr-method', default='step', type=str,\n",
        "                    help='method of learning rate')\n",
        "parser.add_argument('--lr-params', default=[], dest='lr_params',nargs='*',type=float,\n",
        "                    action='append', help='params of lr method')\n",
        "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
        "                    help='momentum')\n",
        "parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float,\n",
        "                    metavar='W', help='weight decay (default: 1e-4)')\n",
        "parser.add_argument('--print-freq', '-p', default=50, type=int,\n",
        "                    metavar='N', help='print frequency (default: 10)')\n",
        "parser.add_argument('--gpu', default=None, type=int,\n",
        "                    help='GPU id to use.')\n",
        "parser.add_argument('--data',default=\"./data/\",\n",
        "                    help='path to dataset')\n",
        "parser.add_argument('--dataset', type=str, default=\"cifar\") \n",
        "parser.add_argument('--modeldir', default=\"imagenet_adv_selfie\", type=str,\n",
        "                    help='director of checkpoint')\n",
        "parser.add_argument('--store-model-everyepoch', dest='store_model_everyepoch', action='store_true',\n",
        "                    help='store checkpoint in every epoch')\n",
        "parser.add_argument('--percent', type=float,\n",
        "                    help=\"Used data percent\", default=1.0)\n",
        "parser.add_argument('--evaluation', action=\"store_true\")\n",
        "parser.add_argument('--classification-model', type=str, default=\"\")\n",
        "parser.add_argument('--split-gpu', action=\"store_true\")\n",
        "parser.add_argument('--resume', action=\"store_true\")\n",
        "parser.add_argument('--finetune', action=\"store_true\")\n",
        "parser.add_argument('--evaluation-selfie', action=\"store_true\")\n",
        "parser.add_argument('--num-classes', type=int, default=10)\n",
        "parser.add_argument('--seed', type=int, default=10)\n",
        "best_prec1 = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1C-Vny1tZTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    global args, best_prec1\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    print(args)\n",
        "    \n",
        "    if not torch.cuda.is_available():\n",
        "        logging.info('no gpu device available')\n",
        "        sys.exit(1)\n",
        "    torch.cuda.set_device(int(args.gpu))\n",
        "\n",
        "    setup_seed(args.seed)\n",
        "\n",
        "    # Data Preprocess\n",
        "    traindir = os.path.join(args.data, 'train')\n",
        "    valdir = os.path.join(args.data, 'val')\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Pad(2),\n",
        "            transforms.RandomCrop(32),\n",
        "            transforms.ToTensor(),\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "    } \n",
        "    if args.dataset == 'cifar':\n",
        "        train_dataset = CifarDataset(args.data, True, data_transforms['train'], args.percent)\n",
        "        test_dataset = CifarDataset(args.data, False, data_transforms['val'], 1)\n",
        "    elif args.dataset == 'imagenet':\n",
        "        train_dataset = ImageNetDataset(args.data, True, data_transforms['train'], args.percent)\n",
        "        test_dataset = ImageNetDataset(args.data, False, data_transforms['val'], 1)\n",
        "\n",
        "    elif args.dataset == 'imagenet224':\n",
        "        data_transforms = {\n",
        "            'train': transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "            ]),\n",
        "            'val': transforms.Compose([\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "        } \n",
        "        train_dataset = datasets.ImageNet(args.data, 'train', True, data_transforms['train'])\n",
        "        test_dataset = datasets.ImageNet(args.data, 'train', True, data_transforms['val'])\n",
        "\n",
        "    valid_size = 0.1\n",
        "    indices = list(range(len(train_dataset)))\n",
        "    split = int(np.floor(valid_size*len(train_dataset)))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "    train_sampler = torch.utils.data.Subset(train_dataset, train_idx)\n",
        "    valid_sampler = torch.utils.data.Subset(train_dataset, valid_idx)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_sampler,\n",
        "        batch_size=args.batch_size, shuffle=True,\n",
        "        num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        valid_sampler,\n",
        "        batch_size=args.batch_size, shuffle=False,\n",
        "        num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=args.batch_size, shuffle=False,\n",
        "        num_workers=args.workers, pin_memory=True)\n",
        "\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().cuda(args.gpu)\n",
        "\n",
        "    # define model \n",
        "    n_split = 4\n",
        "    selfie_model = get_selfie_model(n_split)\n",
        "    selfie_model = selfie_model.cuda()\n",
        "\n",
        "    P=get_P_model()\n",
        "    normalize = NormalizeByChannelMeanStd(\n",
        "    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    P = nn.Sequential(normalize, P)\n",
        "    P = P.cuda()\n",
        "\n",
        "    #define optimizer and scheduler \n",
        "    params_list = [{'params': selfie_model.parameters(), 'lr': args.lr,\n",
        "                        'weight_decay': args.weight_decay},]\n",
        "    params_list.append({'params': P.parameters(), 'lr': args.lr, 'weight_decay': args.weight_decay})\n",
        "    optimizer = torch.optim.SGD(params_list, lr=args.lr,\n",
        "                                momentum=0.9,\n",
        "                                weight_decay=args.weight_decay, nesterov = True)\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "        optimizer,\n",
        "        lr_lambda=lambda step: cosine_annealing(\n",
        "            step,\n",
        "            args.epochs * len(train_loader),\n",
        "            1,  # since lr_lambda computes multiplicative factor\n",
        "            1e-7 / args.lr))\n",
        "\n",
        "    print(\"Training model.\")\n",
        "    step = 0\n",
        "    if os.path.exists(args.modeldir) is not True:\n",
        "        os.mkdir(args.modeldir)\n",
        "    stats_ = stats(args.modeldir, args.start_epoch)\n",
        "\n",
        "    if args.epochs > 0:\n",
        "\n",
        "        #order of patches \n",
        "        all_seq=[np.random.permutation(16) for ind in range(400)]\n",
        "        pickle.dump(all_seq, open(os.path.join(args.modeldir, 'img_test_seq.pkl'),'wb'))\n",
        "        # all_seq=pickle.load(open(os.path.join(args.modeldir, 'img_test_seq.pkl'),'rb'))\n",
        "        \n",
        "        print(\"Begin selfie training...\")\n",
        "        for epoch in range(args.start_epoch, args.epochs):\n",
        "            print(\"The learning rate is {}\".format(optimizer.param_groups[0]['lr']))\n",
        "            trainObj, top1 = train_selfie(train_loader, selfie_model, P, criterion, optimizer, epoch, scheduler)\n",
        "\n",
        "            valObj, prec1 = val_selfie(val_loader, selfie_model, P, criterion, all_seq)\n",
        "\n",
        "            stats_._update(trainObj, top1, valObj, prec1, valObj, prec1)\n",
        "\n",
        "            is_best = prec1 > best_prec1\n",
        "            best_prec1 = max(prec1, best_prec1)\n",
        "\n",
        "            if is_best:\n",
        "                torch.save(\n",
        "                    {\n",
        "                    'epoch': epoch,\n",
        "                    'P_state': P.state_dict(),\n",
        "                    'selfie_state': selfie_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'best_prec1': best_prec1,\n",
        "                    }, os.path.join(args.modeldir, 'std_selfie_TA_model_best.pth.tar'))\n",
        "\n",
        "            torch.save(\n",
        "                    {\n",
        "                    'epoch': epoch,\n",
        "                    'P_state': P.state_dict(),\n",
        "                    'selfie_state': selfie_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'best_prec1': best_prec1,\n",
        "                    }, os.path.join(args.modeldir, 'std_selfie_checkpoint.pth.tar'))\n",
        "\n",
        "            plot_curve(stats_, args.modeldir, True)\n",
        "            data = stats_\n",
        "            sio.savemat(os.path.join(args.modeldir,'stats.mat'), {'data':data})\n",
        "   \n",
        "\n",
        "        print(\"testing TA best selfie model from checkpoint...\")\n",
        "        model_path = os.path.join(args.modeldir, 'std_selfie_TA_model_best.pth.tar')\n",
        "        model_loaded = torch.load(model_path)\n",
        "\n",
        "        P.load_state_dict(model_loaded['P_state'])\n",
        "        selfie_model.load_state_dict(model_loaded['selfie_state'])\n",
        "        print(\"Best TA selfie model loaded! \")\n",
        "        \n",
        "        valObj, prec1 = val_selfie(test_loader, selfie_model, P, criterion, all_seq)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZWAqsEetg5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_selfie(train_loader, selfie_model, P, criterion, optimizer, epoch, scheduler):\n",
        "    global args\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    end = time.time()\n",
        "    selfie_model.train()\n",
        "    P.train()\n",
        "\n",
        "    for index, (input, _) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - end)\n",
        "\n",
        "        cur_batch_size = input.size(0)\n",
        "\n",
        "        # if epoch == 0:\n",
        "        #     warmup_lr(index, optimizer, 200)\n",
        "        \n",
        "        total=16\n",
        "        seq = np.random.permutation(total)\n",
        "        t = seq[:(total // 4)]\n",
        "        v = seq[(total // 4):]\n",
        "        v = torch.from_numpy(v).cuda()\n",
        "        pos = t\n",
        "        t = torch.from_numpy(np.array(pos)).cuda()\n",
        "\n",
        "        input = input.cuda()\n",
        "\n",
        "        #selfie forward\n",
        "        batches = split_image_selfie(input, 8)\n",
        "\n",
        "        batches = list(map(lambda x: x.unsqueeze(1), batches))\n",
        "        batches = torch.cat(batches, 1) # (B, L, C, H, W)\n",
        "\n",
        "        input_batches = torch.split(batches, 1, 1)\n",
        "        input_batches = list(map(lambda x: x.squeeze(1), input_batches))\n",
        "        input_batches = torch.cat(input_batches, 0)\n",
        "\n",
        "        output_batches = P(input_batches)\n",
        "\n",
        "        output_batches = output_batches.unsqueeze(1)\n",
        "        output_batches = torch.split(output_batches, cur_batch_size, 0)\n",
        "        output_batches = torch.cat(output_batches,1)\n",
        "\n",
        "        output_decoder = output_batches.index_select(1, t)\n",
        "        \n",
        "        output_encoder = output_batches.index_select(1, v)\n",
        "        output_encoder = selfie_model(output_encoder, pos)\n",
        "\n",
        "        features = []\n",
        "        for i in range(len(pos)):\n",
        "            feature = output_decoder[:, i, :]\n",
        "            feature = feature.unsqueeze(2)\n",
        "            features.append(feature)\n",
        "\n",
        "        features = torch.cat(features, 2) # (B, F, NP)\n",
        "        patch_loss = 0\n",
        "\n",
        "        for i in range(len(t)):\n",
        "            activate = output_encoder[:, i, :].unsqueeze(1)\n",
        "            pre = torch.bmm(activate, features)\n",
        "            logit = nn.functional.softmax(pre, 2).view(-1, len(t))\n",
        "            temptarget = torch.ones(logit.shape[0]).cuda() * i\n",
        "            temptarget = temptarget.long()\n",
        "            loss_ = criterion(logit, temptarget)\n",
        "            patch_loss += loss_\n",
        "            prec1_adv, _ = accuracy(logit, temptarget, topk=(1,3))\n",
        "            top1.update(prec1_adv[0], 1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        patch_loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        all_loss = patch_loss.float()\n",
        "        losses.update(all_loss.item(), input.size(0))\n",
        "\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        if index % args.print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'.format(\n",
        "                   epoch, index, len(train_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses, top1=top1))\n",
        "    return losses.avg, top1.avg\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma_D0SBdtnHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def val_selfie(val_loader, selfie_model, P, criterion, all_seq):\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top1 = AverageMeter()\n",
        "    end = time.time()\n",
        "    selfie_model.eval()\n",
        "    P.eval()\n",
        "    with torch.no_grad():\n",
        "        for index, (input, _) in enumerate(val_loader):\n",
        "            #print(input)\n",
        "            data_time.update(time.time() - end)\n",
        "            input = input.cuda()\n",
        "\n",
        "            cur_batch_size = input.size(0)\n",
        "\n",
        "            total=16\n",
        "            seq = all_seq[index]\n",
        "            t = seq[:(total // 4)]\n",
        "            v = seq[(total // 4):]\n",
        "            v = torch.from_numpy(v).cuda()\n",
        "            pos = t\n",
        "\n",
        "            t = torch.from_numpy(np.array(pos)).cuda()\n",
        "\n",
        "            #selfie forward\n",
        "            batches = split_image_selfie(input, 8)\n",
        "\n",
        "            batches = list(map(lambda x: x.unsqueeze(1), batches))\n",
        "            batches = torch.cat(batches, 1) # (B, L, C, H, W)\n",
        "\n",
        "            input_batches = torch.split(batches, 1, 1)\n",
        "            input_batches = list(map(lambda x: x.squeeze(1), input_batches))\n",
        "            input_batches = torch.cat(input_batches, 0)\n",
        "\n",
        "            output_batches = P(input_batches)\n",
        "\n",
        "            output_batches = output_batches.unsqueeze(1)\n",
        "            output_batches = torch.split(output_batches, cur_batch_size, 0)\n",
        "            output_batches = torch.cat(output_batches,1)\n",
        "\n",
        "            output_decoder = output_batches.index_select(1, t)\n",
        "            \n",
        "            output_encoder = output_batches.index_select(1, v)\n",
        "            output_encoder = selfie_model(output_encoder, pos)\n",
        "\n",
        "            features = []\n",
        "            for i in range(len(pos)):\n",
        "                feature = output_decoder[:, i, :]\n",
        "                feature = feature.unsqueeze(2)\n",
        "                features.append(feature)\n",
        "\n",
        "            features = torch.cat(features, 2) # (B, F, NP)\n",
        "            patch_loss = 0\n",
        "\n",
        "            for i in range(len(t)):\n",
        "                activate = output_encoder[:, i, :].unsqueeze(1)\n",
        "                pre = torch.bmm(activate, features)\n",
        "                logit = nn.functional.softmax(pre, 2).view(-1, len(t))\n",
        "                temptarget = torch.ones(logit.shape[0]).cuda() * i\n",
        "                temptarget = temptarget.long()\n",
        "                loss_ = criterion(logit, temptarget)\n",
        "\n",
        "                prec1, _ = accuracy(logit, temptarget, topk=(1,3))\n",
        "\n",
        "                losses.update(loss_.item(), 1)\n",
        "                top1.update(prec1[0], 1)\n",
        "\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if index % args.print_freq == 0:\n",
        "                print('Test: [{0}/{1}]\\t'\n",
        "                          'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                          'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                          'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'.format(\n",
        "                           index, len(val_loader), batch_time=batch_time, loss=losses,\n",
        "                           top1=top1))\n",
        "            #raise NotImplementedError\n",
        "        print(' * Prec@1 {top1.avg:.3f}'\n",
        "              .format(top1=top1))\n",
        "        return losses.avg, top1.avg\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3RhL3c5tq9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utils\n",
        "\n",
        "def get_selfie_model(n_split):\n",
        "    n_layers = 12\n",
        "    d_model = 1024 #vector length after the patch routed in P\n",
        "    d_in = 64\n",
        "    n_heads = d_model// d_in\n",
        "    d_ff = 2048\n",
        "    model = SelfieModel(n_layers, n_heads, d_in, d_model, d_ff, n_split)\n",
        "    return model\n",
        "\n",
        "def cosine_annealing(step, total_steps, lr_max, lr_min):\n",
        "    return lr_min + (lr_max - lr_min) * 0.5 * (\n",
        "            1 + np.cos(step / total_steps * np.pi))\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    #print(output.shape)\n",
        "    #print(target.shape)\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "        #print(target)\n",
        "        if (target.dim() > 1):\n",
        "            target = torch.argmax(target, 1)\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, filename[0])\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename[0], filename[1])\n",
        "\n",
        "def setup_seed(seed): \n",
        "    torch.manual_seed(seed) \n",
        "    torch.cuda.manual_seed_all(seed) \n",
        "    np.random.seed(seed) \n",
        "    random.seed(seed) \n",
        "    torch.backends.cudnn.deterministic = True \n",
        "\n",
        "def warmup_lr(step, optimizer, speed):\n",
        "    lr = 0.01+step*(0.1-0.01)/speed\n",
        "    lr = min(lr,0.1)\n",
        "    for p in optimizer.param_groups:\n",
        "        p['lr']=lr\n",
        "\n",
        "def split_image_selfie(image, N):\n",
        "    \"\"\"\n",
        "    image: (B, C, W, H)\n",
        "    \"\"\"\n",
        "    batches = []\n",
        "\n",
        "    for i in list(torch.split(image, N, dim=2)):\n",
        "        batches.extend(list(torch.split(i, N, dim=3)))\n",
        "\n",
        "    return batches\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qmXca7ityrR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}