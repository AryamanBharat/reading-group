{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Adversarial Examples and Generalization in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten, Activation\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "use_cuda=True\n",
    "\n",
    "#Loading Data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Zero', 'One', 'Two', 'Three', 'Four', 'Five', 'Six', 'Seven', 'Eight', 'Nine']\n",
    "#Reshaping and One hot ecoding data\n",
    "img_rows, img_cols, channels = 28, 28, 1\n",
    "num_classes = 10\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "x_train = x_train.reshape((-1, img_rows, img_cols, channels))\n",
    "x_test = x_test.reshape((-1, img_rows, img_cols, channels))\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#print(\"Data shapes\", x_test.shape, y_test.shape, x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu', input_shape=(img_rows, img_cols, channels)))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), strides=(3, 3), padding='same', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 8s 128us/sample - loss: 0.0191 - accuracy: 0.8675 - val_loss: 0.0060 - val_accuracy: 0.9603\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0089 - accuracy: 0.9424 - val_loss: 0.0049 - val_accuracy: 0.9689\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.0073 - accuracy: 0.9532 - val_loss: 0.0041 - val_accuracy: 0.9734\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0062 - accuracy: 0.9603 - val_loss: 0.0042 - val_accuracy: 0.9730\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 9s 148us/sample - loss: 0.0057 - accuracy: 0.9635 - val_loss: 0.0036 - val_accuracy: 0.9762\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.0052 - accuracy: 0.9668 - val_loss: 0.0034 - val_accuracy: 0.9785\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.0048 - accuracy: 0.9694 - val_loss: 0.0033 - val_accuracy: 0.9791\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.0045 - accuracy: 0.9710 - val_loss: 0.0032 - val_accuracy: 0.9796\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.0041 - accuracy: 0.9736 - val_loss: 0.0033 - val_accuracy: 0.9788\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 7s 121us/sample - loss: 0.0040 - accuracy: 0.9750 - val_loss: 0.0035 - val_accuracy: 0.9776\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 7s 117us/sample - loss: 0.0037 - accuracy: 0.9765 - val_loss: 0.0035 - val_accuracy: 0.9787\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 7s 122us/sample - loss: 0.0035 - accuracy: 0.9778 - val_loss: 0.0029 - val_accuracy: 0.9818\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 7s 109us/sample - loss: 0.0035 - accuracy: 0.9780 - val_loss: 0.0031 - val_accuracy: 0.9803\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.0035 - accuracy: 0.9784 - val_loss: 0.0033 - val_accuracy: 0.9808\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0034 - accuracy: 0.9792 - val_loss: 0.0031 - val_accuracy: 0.9811\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 7s 111us/sample - loss: 0.0032 - accuracy: 0.9801 - val_loss: 0.0027 - val_accuracy: 0.9821\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 7s 115us/sample - loss: 0.0032 - accuracy: 0.9803 - val_loss: 0.0030 - val_accuracy: 0.9823\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0031 - accuracy: 0.9810 - val_loss: 0.0029 - val_accuracy: 0.9813\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0030 - accuracy: 0.9814 - val_loss: 0.0029 - val_accuracy: 0.9833\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0029 - accuracy: 0.9818 - val_loss: 0.0029 - val_accuracy: 0.9820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x235919cfdc8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Accuracy without any training and testing on Adversarial examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy on regular images: [0.0029356633863323023, 0.982]\n"
     ]
    }
   ],
   "source": [
    " print(\"Base accuracy on regular images:\", model.evaluate(x=x_test, y=y_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to generate the perturbation will take in an image to apply perturbations to, and the label that correctly classifies the image. We can set up the gradients to calculate. We will be doing this using a tf.GradientTape() object. The GradientTape object keeps track of what happens to a tensor that it’s “watching,” and can automatically calculate the gradients for it. For more details on tf.GradientTape() object refer this https://medium.com/analytics-vidhya/tf-gradienttape-explained-for-keras-users-cc3f06276f22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to crate an Adversarial example\n",
    "def adversarial_pattern(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        prediction = model(image)\n",
    "        loss = tf.keras.losses.MSE(label, prediction)\n",
    "    \n",
    "    gradient = tape.gradient(loss, image)\n",
    "    \n",
    "    signed_grad = tf.sign(gradient)\n",
    "    \n",
    "    return signed_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Adversarial Example\n",
    "image = x_train[1]\n",
    "image_label = y_train[1]\n",
    "perturbations = adversarial_pattern(image.reshape((1, img_rows, img_cols, channels)), image_label).numpy()\n",
    "#plt.imshow(image.reshape(img_rows,img_cols))\n",
    "#plt.show()\n",
    "\n",
    "#output before noise\n",
    "#print(labels[model.predict(image.reshape((1, img_rows, img_cols, channels))).argmax()])\n",
    "\n",
    "eps = 0.2\n",
    "adversarial = image + perturbations * eps\n",
    "\n",
    "if channels == 1:\n",
    "    plt.imshow(adversarial.reshape((img_rows, img_cols)))\n",
    "    \n",
    "else:\n",
    "    plt.imshow(adversarial.reshape((img_rows, img_cols, channels)))\n",
    "\n",
    "plt.title(\"Original {} -> Predicted {}\".format(labels[model.predict(image.reshape((1, img_rows, img_cols, channels))).argmax()], labels[model.predict(adversarial).argmax()]))\n",
    "plt.show()\n",
    "\n",
    "#output after noise\n",
    "#print(labels[model.predict(adversarial).argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Generalize we need an Adversarial Example Generator and then train our pretrained model on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adversarials(batch_size):\n",
    "    while True:\n",
    "        x = []\n",
    "        y = []\n",
    "        for batch in range(batch_size):\n",
    "            N = random.randint(0, 100)\n",
    "\n",
    "            label = y_train[N]\n",
    "            image = x_train[N]\n",
    "            \n",
    "            perturbations = adversarial_pattern(image.reshape((1, img_rows, img_cols, channels)), label).numpy()\n",
    "            \n",
    "            \n",
    "            epsilon = 0.2\n",
    "            adversarial = image + perturbations * epsilon\n",
    "            \n",
    "            x.append(adversarial)\n",
    "            y.append(y_train[N])\n",
    "        \n",
    "        \n",
    "        x = np.asarray(x).reshape((batch_size, img_rows, img_cols, channels))\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of Model without any training on Adversarial Examples but testing on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy on adversarial images: [0.1859811827659607, 0.0078]\n"
     ]
    }
   ],
   "source": [
    "x_adversarial_test, y_adversarial_test = next(generate_adversarials(10000))\n",
    "print(\"Base accuracy on adversarial images:\", model.evaluate(x=x_adversarial_test, y=y_adversarial_test, verbose=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy of Model after training the pre trained model on Adversarial Examples generated using the generator. And testing on the normal testing data (Not including Adversarial examples ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.0245 - accuracy: 0.8585 - val_loss: 0.0084 - val_accuracy: 0.9469\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.0018 - accuracy: 0.9902 - val_loss: 0.0092 - val_accuracy: 0.9411\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 6.4039e-04 - accuracy: 0.9964 - val_loss: 0.0088 - val_accuracy: 0.9434\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 5.8587e-05 - accuracy: 0.9998 - val_loss: 0.0089 - val_accuracy: 0.9443\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 7.5499e-05 - accuracy: 0.9996 - val_loss: 0.0117 - val_accuracy: 0.9261\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 1.0996e-04 - accuracy: 0.9995 - val_loss: 0.0102 - val_accuracy: 0.9382\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 3s 149us/sample - loss: 3.7675e-05 - accuracy: 0.9998 - val_loss: 0.0093 - val_accuracy: 0.9442\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 3s 146us/sample - loss: 7.4051e-05 - accuracy: 0.9995 - val_loss: 0.0133 - val_accuracy: 0.9226\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 1.3673e-04 - accuracy: 0.9992 - val_loss: 0.0125 - val_accuracy: 0.9266\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 3s 148us/sample - loss: 1.3997e-04 - accuracy: 0.9991 - val_loss: 0.0132 - val_accuracy: 0.9248\n",
      "Defended accuracy on adversarial images: [8.456839683161419e-16, 1.0]\n",
      "Defended accuracy on regular images: [0.013161495559072848, 0.9248]\n"
     ]
    }
   ],
   "source": [
    "# Training the same model using adversail generator\n",
    "# We can either train the model directly on the generator or pre-generate a dataset of adversarial images to fit on. Training the model on a generator allows new images to be made on-the-fly to fool the model, which would mean that the model would learn to adapt to more complex images, and become more robust in the process. Despite these benefits, this method requires significantly more computing power, and for the sake of this tutorial, we are going to pre-generate a dataset of adversarial images.\n",
    "x_adversarial_train, y_adversarial_train = next(generate_adversarials(20000))\n",
    "model.fit(x_adversarial_train, y_adversarial_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))\n",
    "print(\"Defended accuracy on adversarial images:\", model.evaluate(x=x_adversarial_test, y=y_adversarial_test, verbose=0))\n",
    "print(\"Defended accuracy on regular images:\", model.evaluate(x=x_test, y=y_test, verbose=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is a noticeable drop in accuracy when evaluated on regular images, the defended model performs incredibly on the images that once fooled it. Although the model can defeat the images that used to fool it, we can still generate new images based on its weaknesses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
